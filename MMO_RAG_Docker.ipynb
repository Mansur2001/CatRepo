{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version uses Milvus through Docker Compose so you must have Docker installed to run this notebook (Milvus is spun up via `docker compose up -d` as shown in the block below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymilvus milvus langchain sentence-transformers tiktoken octoai-sdk\n",
    "# docker compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.octoai_endpoint import OctoAIEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OCTOAI_API_TOKEN\"] = os.getenv(\"OCTOAI_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n Instruction:\\n{question}\\n Response: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OctoAIEndpoint(\n",
    "    endpoint_url=\"https://text.octoai.run/v1/chat/completions\",\n",
    "    model_kwargs={\n",
    "        \"model\": \"mixtral-8x7b-instruct-fp16\",\n",
    "        \"max_tokens\": 128,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"temperature\": 0.01,\n",
    "        \"top_p\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Keep your responses limited to one short paragraph if possible.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Leonardo da Vinci (1452-1519) was an Italian polymath who is often regarded as one of the greatest painters in history. He is also celebrated for his technological ingenuity, scientific curiosity, and philosophical wisdom. Da Vinci is widely known for his masterpieces such as 'The Last Supper' and 'Mona Lisa.' As an artist, scientist, mathematician, engineer, inventor, anatomist, geologist, cartographer, botanist, musician, and writer, da Vinci embodied the Renaissance ideal. His thirst for\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was leonardo davinci?\"\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.invoke(question)[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OctoAIEmbeddings\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OctoAIEmbeddings(endpoint_url=\"https://text.octoai.run/v1/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"./ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1007, which is longer than the specified 512\n",
      "Created a chunk of size 599, which is longer than the specified 512\n",
      "Created a chunk of size 672, which is longer than the specified 512\n",
      "Created a chunk of size 657, which is longer than the specified 512\n",
      "Created a chunk of size 799, which is longer than the specified 512\n",
      "Created a chunk of size 876, which is longer than the specified 512\n",
      "Created a chunk of size 1111, which is longer than the specified 512\n",
      "Created a chunk of size 933, which is longer than the specified 512\n",
      "Created a chunk of size 1341, which is longer than the specified 512\n",
      "Created a chunk of size 1339, which is longer than the specified 512\n",
      "Created a chunk of size 1287, which is longer than the specified 512\n",
      "Created a chunk of size 1538, which is longer than the specified 512\n",
      "Created a chunk of size 1674, which is longer than the specified 512\n",
      "Created a chunk of size 593, which is longer than the specified 512\n",
      "Created a chunk of size 526, which is longer than the specified 512\n",
      "Created a chunk of size 622, which is longer than the specified 512\n",
      "Created a chunk of size 551, which is longer than the specified 512\n",
      "Created a chunk of size 551, which is longer than the specified 512\n",
      "Created a chunk of size 1244, which is longer than the specified 512\n",
      "Created a chunk of size 597, which is longer than the specified 512\n",
      "Created a chunk of size 526, which is longer than the specified 512\n",
      "Created a chunk of size 724, which is longer than the specified 512\n",
      "Created a chunk of size 871, which is longer than the specified 512\n",
      "Created a chunk of size 900, which is longer than the specified 512\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    with open(f\"./ascii/{file}\", encoding=\"utf-8\") as f:\n",
    "        file_text = f.read()\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512, chunk_overlap=64, \n",
    "    )\n",
    "    texts = text_splitter.split_text(file_text)\n",
    "    for i, chunked_text in enumerate(texts):\n",
    "        file_texts.append(Document(page_content=chunked_text, \n",
    "                metadata={\"doc_title\": file.split(\".\")[0], \"chunk_num\": i}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Milvus.from_documents(\n",
    "    file_texts,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": 19530},\n",
    "    collection_name=\"cities\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Celtic cross by Joan Stark\\n\\n         _..._\\n       .-|>X<|-.\\n     _//`|oxo|`\\\\\\\\_  \\n    /xo=._\\\\X/_.=ox\\\\\\n    |<>X<>(_)<>X<>|\\n    \\\\xo.=\\'/X\\\\\\'=.ox/\\n      \\\\\\\\_/oxo\\\\_//\\n       \\';<>X<>;\\'\\n        |=====|\\n        |<>X<>|\\n        |oxoxo|\\n        |<>X<>|\\n       _|oxoxo|_\\njgs.--\\' `\"\"\"\"\"` \\'--.\\n\\nCeltic knots\\n\\n   /\\\\  /\\\\\\n  /  \\\\/  \\\\\\n / /\\\\ \\\\/\\\\ \\\\\\n \\\\ \\\\/\\\\ \\\\/ /\\n  \\\\/ /\\\\/ /\\n  / /\\\\/ /\\\\\\n / /\\\\  /\\\\ \\\\\\n/ /  \\\\/  \\\\ \\\\\\n\\\\ \\\\  /\\\\  / /\\n \\\\ \\\\/  \\\\/ /\\n  \\\\/ /\\\\/ /\\n  / /\\\\/ /\\\\\\n / /\\\\ \\\\/\\\\ \\\\\\n \\\\ \\\\/\\\\ \\\\/ /\\n  \\\\  /\\\\  /\\n   \\\\/  \\\\/', metadata={'doc_title': 'celtic', 'chunk_num': 0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a simple ASCII representation of a village in the mountains:\\n\\n    _/_/\\n   /   \\\\\\n__/     \\\\__\\n\\\\         /\\\\\\n_\\\\       _/\\n  \\\\     /\\n___\\\\ /_/__\\n\\\\_____/\\n \\nThis representation features a mountain range with a village nestled in the valley. Please note that this is a very basic representation and can be further customized based on your specific needs.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"can you create an ascii representation of a village in the mountains?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this a bit more fun and showcase the multilingual capabilities of Mixtal which really outshine other open source models\n",
    "\n",
    "# Our Vector DB is populated with entries from english text - even the embedding model we're using here, GTE-Large\n",
    "# works best on english text. However Mixtral has good mutlilingual capabilities in French, German, Spanish and Italian.\n",
    "# So what we'll do is ask the assistant to only answer in french in the system and user prompt. RAG here is performed based on \n",
    "# english text, but upon producing the user response, the Mixtral LLM will generate tokens in a different language here (french)\n",
    "french_llm = OctoAIEndpoint(\n",
    "    endpoint_url=\"https://text.octoai.run/v1/chat/completions\",\n",
    "    model_kwargs={\n",
    "        \"model\": \"mixtral-8x7b-instruct-fp16\",\n",
    "        \"max_tokens\": 128,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.9,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant who responds in French and not in English.\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "french_template = \"\"\"Answer the question in French based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "french_prompt = PromptTemplate.from_template(french_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | french_prompt\n",
    "    | french_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_1 = french_chain.invoke(\"How big is the city of Seattle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' La ville de Seattle est assez grande avec une population de 749 256 '\n",
      " \"habitants en 2022. C'est la ville la plus peuplée de l'État de Washington et \"\n",
      " \"de la région du Nord-Ouest Pacifique de l'Amérique du Nord. L'aire \"\n",
      " \"métropolitaine de Seattle compte 4,02 millions d'habitants, ce qui en fait \"\n",
      " 'la 15e plus importante aux États-Unis. La croissance de la population de '\n",
      " 'Seattle a été rapide, avec une augmentation de 21,1%')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(fr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
